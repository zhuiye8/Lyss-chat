# 记忆服务技术选型调研报告 (V3.2 最终版)

## 1. 概述

本文档旨在为 Lyss AI Platform 的记忆服务选择一个合适的、可独立部署的开源引擎，并设计一套能完全适配本平台业务场景的实施方案。本方案已包含对**会话内短期记忆**和**会话间长期记忆**的综合考虑。

**核心目标**:
1.  **有效降低成本**: 最大限度地减少对LLM API的重复调用。
2.  **尊重用户意图**: 保证用户在切换模型进行对比测试时，能获得真实、无污染的回答。
3.  **提供用户选择权**: 允许用户根据自己的需求，自主决定是否启用缓存/记忆功能。
4.  **高效管理上下文**: 智能、动态地处理长对话的上下文，避免token超限。

## 2. 记忆的两个维度：会话内与会话间

### 2.1. 会话内短期记忆 (In-Session Context)

- **问题**: 在一次连续的对话中，如何高效地传递上下文，避免token超限和成本激增？
- **V1.0方案**: **动态滑动窗口 (Dynamic Sliding Window)**。
    - **原理**: 在每次请求前，动态计算当前模型剩余的可用上下文空间（`模型上限 - 预留回答空间 - 新问题Token数`），然后从最新的对话历史开始，向前逐条填充，直到填满可用空间。
    - **优点**: 相比固定的N轮对话，此方法能最大限度地利用模型的上下文能力，更智能、更高效。
    - **实现**: 此逻辑由**网关层**负责，不依赖于专门的记忆服务。
- **V2.0方案**: **三级分层记忆模型**。
    - **原理**: 将对话历史分为“近期（完整保留）”、“中期（要点提取）”、“远期（高度概括）”三个层次，通过`mem0.ai`或类似的摘要模型进行智能压缩，实现更长期的、高质量的对话记忆。
    - **用户告知**: 在UI上提供一个非干扰性的状态指示器，告知用户当前上下文的完整状态。

### 2.2. 会话间长期记忆/缓存 (Cross-Session Memory/Cache)

- **问题**: 如何记住用户跨越不同对话的偏好或缓存重复问题的答案，以降低成本、提升体验？
- **V1.0方案**: **带精细化控制的 GPTCache**。

## 3. GPTCache 实施方案

### 3.1. 技术选型：GPTCache

我们V1.0阶段选择 **`GPTCache`** 作为跨会话记忆的实现方案，因为它最直接地满足了“降低成本”的核心目标。

### 3.2. 核心设计：精细化缓存与用户控制

为了克服`GPTCache`的固有缺陷并满足我们的业务需求，必须实施以下设计：

#### 3.2.1. 精细化缓存键

`GPTCache`的缓存**绝不能**仅基于问题（`prompt`）本身。缓存的唯一标识（Key）必须是一个**复合结构**，至少包含以下三个维度：

1.  **`user_id`**: 缓存必须是用户隔离的。
2.  **`model_name`**: 缓存必须是模型隔离的，以支持多模型对比场景。
3.  **`prompt_embedding`**: 问题本身的语义向量。

#### 3.2.2. 用户偏好开关

- **数据库设计**: 在 `users` 表中增加一个新字段：`cache_enabled` (BOOLEAN, DEFAULT: true)。
- **前端UI**: 在用户设置界面，提供一个对应的开关，允许用户随时开启或关闭`GPTCache`功能。

### 3.3. 服务端逻辑流程

网关服务在处理请求时，会先检查用户的`cache_enabled`设置，然后使用复合Key进行缓存查询。如果未命中，则在调用LLM获取新回答后，异步地将结果写回缓存。

## 4. 结论

此最终方案通过**动态滑动窗口**解决了V1.0的会话内上下文管理问题，并通过**精细化缓存键**和**用户控制开关**两大核心设计，使`GPTCache`能够完美融入我们多供应商、用户驱动的平台架构中。这是一个兼顾了成本、用户体验和技术扩展性的健壮方案。